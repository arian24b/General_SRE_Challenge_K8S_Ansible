name: CI/CD Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      deploy_infrastructure:
        description: "Deploy infrastructure"
        type: boolean
        default: false

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository_owner }}/general-sre-challenge-k8s-ansible-api

jobs:
  # ============= CONTINUOUS INTEGRATION =============
  lint-and-test:
    name: Lint & Test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Install dependencies
        run: |
          cd application/api
          uv sync --frozen

      - name: Run linting
        run: |
          cd application/api
          uv run ruff check .

      - name: Run tests
        run: |
          cd application/api
          uv run pytest --cov=app --cov-report=xml --cov-report=term

      - name: Upload coverage
        if: always()
        uses: codecov/codecov-action@v5
        with:
          file: ./application/api/coverage.xml
          flags: unittests

  validate-manifests:
    name: Validate Kubernetes Manifests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: "v3.14.0"

      - name: Lint Helm charts
        run: |
          cd application/helm
          helm dependency update
          helm lint .

      - name: Validate Helm template
        run: |
          cd application/helm
          helm template test . --debug --dry-run

      - name: Validate Ansible syntax
        run: |
          pip install ansible
          cd infrastructure/ansible
          ansible-playbook --syntax-check provision-infrastructure.yml
          ansible-playbook --syntax-check setup-cluster.yml

  # ============= BUILD & PUSH =============
  build-and-push:
    name: Build & Push Docker Image
    needs: [lint-and-test, validate-manifests]
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      security-events: write
    outputs:
      image-tag: ${{ steps.meta.outputs.version }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: ./application/api
          push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Run Trivy security scan
        id: trivy
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: aquasecurity/trivy-action@0.33.1
        with:
          image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          format: "sarif"
          output: "trivy-results.sarif"

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v4
        if: always() && steps.trivy.outcome != 'skipped'
        with:
          sarif_file: "trivy-results.sarif"

  # ============= INFRASTRUCTURE DEPLOYMENT =============
  deploy-infrastructure:
    name: Deploy Infrastructure
    needs: build-and-push
    if: github.event.inputs.deploy_infrastructure == 'true'
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Setup Ansible
        run: pip install ansible

      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_ed25519
          chmod 600 ~/.ssh/id_ed25519
          ssh-keyscan -H github.com >> ~/.ssh/known_hosts || true
          # Generate public key from private key for cloud-init injection
          ssh-keygen -y -f ~/.ssh/id_ed25519 > ~/.ssh/id_ed25519.pub

      - name: Terraform Init
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARVAN_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARVAN_S3_SECRET_KEY }}
          AWS_EC2_METADATA_DISABLED: "true"
        run: |
          cd infrastructure/terraform
          terraform init -reconfigure

      - name: Terraform Plan
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARVAN_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARVAN_S3_SECRET_KEY }}
          AWS_EC2_METADATA_DISABLED: "true"
        run: |
          cd infrastructure/terraform
          SSH_PUBLIC_KEY=$(cat ~/.ssh/id_ed25519.pub)
          terraform plan \
            -var="arvan_api_key=${{ secrets.ARVAN_API_KEY }}" \
            -var="ssh_public_key=${SSH_PUBLIC_KEY}" \
            -out=tfplan

      - name: Terraform Apply
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARVAN_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARVAN_S3_SECRET_KEY }}
          AWS_EC2_METADATA_DISABLED: "true"
        run: |
          cd infrastructure/terraform
          terraform apply -auto-approve tfplan

      - name: Get Public IPs from ArvanCloud API
        env:
          ARVAN_API_KEY: ${{ secrets.ARVAN_API_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.ARVAN_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARVAN_S3_SECRET_KEY }}
          AWS_EC2_METADATA_DISABLED: "true"
        run: |
          cd infrastructure/terraform

          # Get server IDs from terraform
          MASTER_ID=$(terraform output -raw master_id)
          WORKER_IDS=$(terraform output -json worker_ids | jq -r '.[]')
          REGION=$(terraform output -raw region 2>/dev/null || echo "ir-thr-c2")

          echo "Master ID: $MASTER_ID"
          echo "Worker IDs: $WORKER_IDS"

          # Function to get public IP from ArvanCloud API
          get_public_ip() {
            local server_id=$1
            local response=$(curl -s -H "Authorization: $ARVAN_API_KEY" \
              "https://napi.arvancloud.ir/ecc/v1/regions/${REGION}/servers/${server_id}")

            # Extract public IPv4 address
            local public_ip=$(echo "$response" | jq -r '.data.addresses | to_entries[] | .value[] | select(.version == "4" and .is_public == true) | .addr' | head -1)

            if [ -z "$public_ip" ] || [ "$public_ip" == "null" ]; then
              # Fallback: try to get any public address
              public_ip=$(echo "$response" | jq -r '.data.addresses | to_entries[] | .value[] | select(.is_public == true) | .addr' | head -1)
            fi

            echo "$public_ip"
          }

          # Get master public IP and password
          MASTER_PUBLIC_IP=$(get_public_ip "$MASTER_ID")
          MASTER_PASSWORD=$(terraform state show module.compute.arvan_abrak.k8s_master | grep 'password' | awk -F'"' '{print $2}')
          echo "Master Public IP: $MASTER_PUBLIC_IP"

          # Get worker public IPs and passwords
          WORKER_PUBLIC_IPS=""
          WORKER_PASSWORDS=""
          worker_index=0
          for worker_id in $WORKER_IDS; do
            worker_ip=$(get_public_ip "$worker_id")
            worker_password=$(terraform state show "module.compute.arvan_abrak.k8s_worker[$worker_index]" | grep 'password' | awk -F'"' '{print $2}')
            echo "Worker $worker_index Public IP: $worker_ip"

            if [ -n "$WORKER_PUBLIC_IPS" ]; then
              WORKER_PUBLIC_IPS="$WORKER_PUBLIC_IPS,$worker_ip"
              WORKER_PASSWORDS="$WORKER_PASSWORDS,$worker_password"
            else
              WORKER_PUBLIC_IPS="$worker_ip"
              WORKER_PASSWORDS="$worker_password"
            fi
            worker_index=$((worker_index + 1))
          done

          # Save IPs and passwords to files for later steps
          echo "$MASTER_PUBLIC_IP" > /tmp/master_public_ip
          echo "$MASTER_PASSWORD" > /tmp/master_password
          echo "$WORKER_PUBLIC_IPS" > /tmp/worker_public_ips
          echo "$WORKER_PASSWORDS" > /tmp/worker_passwords

          echo "=== Public IPs ==="
          echo "Master: $MASTER_PUBLIC_IP"
          echo "Workers: $WORKER_PUBLIC_IPS"

      - name: Wait for servers to be reachable
        run: |
          # Read IPs from files
          MASTER_IP=$(cat /tmp/master_public_ip)
          WORKER_IPS=$(cat /tmp/worker_public_ips | tr ',' '\n')

          echo "Master public IP: $MASTER_IP"
          echo "Worker public IPs: $WORKER_IPS"

          # Function to wait for a server to be reachable via SSH
          wait_for_ssh() {
            local ip=$1
            local name=$2
            echo "Waiting for $name ($ip) to be reachable via SSH..."
            for i in {1..60}; do
              if ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes ubuntu@$ip "echo 'SSH connection successful'" 2>/dev/null; then
                echo "$name is reachable via SSH!"
                return 0
              fi
              echo "Attempt $i/60: $name not reachable yet, waiting..."
              sleep 10
            done
            echo "WARNING: $name may not be fully ready after 10 minutes"
            return 1
          }

          # Wait for master node
          wait_for_ssh "$MASTER_IP" "Master node"

          # Wait for each worker node
          for WORKER_IP in $WORKER_IPS; do
            if [ -n "$WORKER_IP" ]; then
              wait_for_ssh "$WORKER_IP" "Worker node"
            fi
          done

          echo "All servers are reachable. Waiting additional 30 seconds for system initialization..."
          sleep 30

      - name: Generate Ansible Inventory
        run: |
          # Read IPs from files
          MASTER_IP=$(cat /tmp/master_public_ip)
          WORKER_IPS=$(cat /tmp/worker_public_ips)

          echo "=== Generating Ansible Inventory ==="
          echo "Master IP: $MASTER_IP"
          echo "Worker IPs: $WORKER_IPS"

          cd infrastructure/ansible

          # Generate inventory file with ubuntu user (SSH key configured via cloud-init)
          # Use 'masters' and 'workers' group names to match playbook expectations
          echo "[masters]" > inventory.ini
          echo "master ansible_host=${MASTER_IP} ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/id_ed25519 ansible_become=yes ansible_become_method=sudo" >> inventory.ini

          echo "" >> inventory.ini
          echo "[workers]" >> inventory.ini

          # Add worker entries
          IFS=',' read -ra WORKER_ARRAY <<< "$WORKER_IPS"
          for i in "${!WORKER_ARRAY[@]}"; do
            echo "worker${i} ansible_host=${WORKER_ARRAY[$i]} ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/id_ed25519 ansible_become=yes ansible_become_method=sudo" >> inventory.ini
          done

          echo "" >> inventory.ini
          echo "[k8s:children]" >> inventory.ini
          echo "masters" >> inventory.ini
          echo "workers" >> inventory.ini

          echo "=== Generated inventory.ini ==="
          cat inventory.ini

      - name: Add hosts to known_hosts
        run: |
          # Read IPs from files
          MASTER_IP=$(cat /tmp/master_public_ip)
          WORKER_IPS=$(cat /tmp/worker_public_ips | tr ',' '\n')

          echo "Adding master ($MASTER_IP) to known_hosts"
          ssh-keyscan -H $MASTER_IP >> ~/.ssh/known_hosts 2>/dev/null || true

          for ip in $WORKER_IPS; do
            if [ -n "$ip" ]; then
              echo "Adding worker ($ip) to known_hosts"
              ssh-keyscan -H $ip >> ~/.ssh/known_hosts 2>/dev/null || true
            fi
          done

      - name: Download Kubernetes binaries on runner
        run: |
          # Download binaries on GitHub Actions runner (not blocked)
          K8S_VERSION=$(curl -L -s https://dl.k8s.io/release/stable.txt)
          echo "Downloading Kubernetes $K8S_VERSION binaries..."

          mkdir -p /tmp/k8s-binaries
          curl -Lo /tmp/k8s-binaries/kubectl "https://dl.k8s.io/release/${K8S_VERSION}/bin/linux/amd64/kubectl"
          curl -Lo /tmp/k8s-binaries/kubeadm "https://dl.k8s.io/release/${K8S_VERSION}/bin/linux/amd64/kubeadm"
          curl -Lo /tmp/k8s-binaries/kubelet "https://dl.k8s.io/release/${K8S_VERSION}/bin/linux/amd64/kubelet"

          chmod +x /tmp/k8s-binaries/*
          ls -la /tmp/k8s-binaries/

          # Also download systemd service files
          curl -Lo /tmp/k8s-binaries/kubelet.service "https://raw.githubusercontent.com/kubernetes/release/v0.16.2/cmd/krel/templates/latest/kubelet/kubelet.service"
          curl -Lo /tmp/k8s-binaries/10-kubeadm.conf "https://raw.githubusercontent.com/kubernetes/release/v0.16.2/cmd/krel/templates/latest/kubeadm/10-kubeadm.conf"

          # Save K8S_VERSION for next steps
          echo "$K8S_VERSION" > /tmp/k8s-version.txt

      - name: Pull and save Kubernetes container images
        run: |
          K8S_VERSION=$(cat /tmp/k8s-version.txt)
          echo "Pulling Kubernetes $K8S_VERSION container images..."

          mkdir -p /tmp/k8s-images

          # List of required images for kubeadm
          IMAGES=(
            "registry.k8s.io/kube-apiserver:${K8S_VERSION}"
            "registry.k8s.io/kube-controller-manager:${K8S_VERSION}"
            "registry.k8s.io/kube-scheduler:${K8S_VERSION}"
            "registry.k8s.io/kube-proxy:${K8S_VERSION}"
            "registry.k8s.io/coredns/coredns:v1.12.0"
            "registry.k8s.io/pause:3.10"
            "registry.k8s.io/etcd:3.5.17-0"
          )

          # Pull and save each image
          for image in "${IMAGES[@]}"; do
            echo "Pulling $image..."
            docker pull "$image"

            # Create a safe filename
            filename=$(echo "$image" | sed 's/[\/:]/_/g')
            echo "Saving to /tmp/k8s-images/${filename}.tar"
            docker save "$image" -o "/tmp/k8s-images/${filename}.tar"
          done

          # Create a compressed archive of all images
          cd /tmp/k8s-images
          tar -czf /tmp/k8s-images.tar.gz *.tar
          ls -lh /tmp/k8s-images.tar.gz

      - name: Copy Kubernetes binaries to all servers
        run: |
          MASTER_IP=$(cat /tmp/master_public_ip)
          WORKER_IPS=$(cat /tmp/worker_public_ips | tr ',' '\n')

          # Copy to master
          echo "Copying binaries to master ($MASTER_IP)..."
          scp -o StrictHostKeyChecking=no /tmp/k8s-binaries/* ubuntu@$MASTER_IP:/tmp/

          # Copy to workers
          for ip in $WORKER_IPS; do
            if [ -n "$ip" ]; then
              echo "Copying binaries to worker ($ip)..."
              scp -o StrictHostKeyChecking=no /tmp/k8s-binaries/* ubuntu@$ip:/tmp/
            fi
          done

      - name: Copy and load Kubernetes images on master
        run: |
          MASTER_IP=$(cat /tmp/master_public_ip)

          echo "Copying images archive to master ($MASTER_IP)..."
          scp -o StrictHostKeyChecking=no /tmp/k8s-images.tar.gz ubuntu@$MASTER_IP:/tmp/

          echo "Extracting and loading images on master..."
          ssh -o StrictHostKeyChecking=no ubuntu@$MASTER_IP << 'EOF'
            cd /tmp
            tar -xzf k8s-images.tar.gz

            echo "Loading images into containerd..."
            for tar_file in *.tar; do
              if [ "$tar_file" != "k8s-images.tar.gz" ]; then
                echo "Loading $tar_file..."
                sudo ctr -n k8s.io images import "$tar_file"
              fi
            done

            echo "Verifying loaded images..."
            sudo ctr -n k8s.io images list

            # Cleanup
            rm -f *.tar k8s-images.tar.gz
          EOF

      - name: Deploy Kubernetes Cluster
        run: |
          cd infrastructure/ansible
          ansible-playbook -i inventory.ini setup-cluster.yml -v

      - name: Verify cluster initialization
        run: |
          MASTER_IP=$(cat /tmp/master_public_ip)
          echo "Verifying cluster initialization on master ($MASTER_IP)..."

          # Check if kubeconfig exists
          if ! ssh -o StrictHostKeyChecking=no ubuntu@$MASTER_IP "sudo test -f /root/.kube/config"; then
            echo "ERROR: Kubeconfig not found at /root/.kube/config"
            echo "Checking for admin.conf..."
            if ssh -o StrictHostKeyChecking=no ubuntu@$MASTER_IP "sudo test -f /etc/kubernetes/admin.conf"; then
              echo "Found /etc/kubernetes/admin.conf, copying to /root/.kube/config"
              ssh -o StrictHostKeyChecking=no ubuntu@$MASTER_IP "sudo mkdir -p /root/.kube && sudo cp /etc/kubernetes/admin.conf /root/.kube/config"
            else
              echo "ERROR: Neither /root/.kube/config nor /etc/kubernetes/admin.conf found"
              echo "Cluster may not have been initialized properly"
              exit 1
            fi
          fi

          # Verify kubeconfig is readable
          ssh -o StrictHostKeyChecking=no ubuntu@$MASTER_IP "sudo cat /root/.kube/config" > /dev/null
          echo "Kubeconfig verified successfully"

      - name: Setup kubectl
        run: |
          mkdir -p ~/.kube
          MASTER_IP=$(cat /tmp/master_public_ip)

          # Copy kubeconfig from master
          echo "Copying kubeconfig from master..."
          ssh -o StrictHostKeyChecking=no ubuntu@$MASTER_IP "sudo cat /root/.kube/config" > ~/.kube/config

          # Replace localhost with master IP
          sed -i "s/127.0.0.1/$MASTER_IP/g" ~/.kube/config

          # Verify kubectl can connect
          echo "Testing kubectl connection..."
          kubectl version --short || true
          kubectl get nodes

      - name: Deploy Monitoring Stack
        run: |
          cd infrastructure/ansible
          ansible-playbook -i inventory.ini deploy-monitoring.yml \
            -e "grafana_admin_password=${{ secrets.GRAFANA_PASSWORD }}"

      - name: Deploy PostgreSQL
        run: |
          cd infrastructure/ansible
          ansible-playbook -i inventory.ini deploy-postgres.yml

  # ============= APPLICATION DEPLOYMENT =============
  deploy-application:
    name: Deploy Application
    needs: build-and-push
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Setup kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > ~/.kube/config

      - name: Setup Helm
        uses: azure/setup-helm@v4

      - name: Create namespace
        run: |
          kubectl create namespace application --dry-run=client -o yaml | kubectl apply -f -

      - name: Create secrets
        run: |
          kubectl create secret generic postgres-secret \
            --from-literal=username="${{ secrets.DB_USERNAME }}" \
            --from-literal=password="${{ secrets.DB_PASSWORD }}" \
            --from-literal=database="${{ secrets.DB_NAME }}" \
            --namespace=application \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy application with Helm
        run: |
          cd application/helm
          helm dependency update
          helm upgrade --install api . \
            --namespace application \
            --set image.repository=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} \
            --set image.tag=latest \
            --set database.host=postgres-service \
            --set database.port=5432 \
            --wait --timeout 5m

      - name: Verify deployment
        run: |
          kubectl rollout status deployment/api -n application --timeout=5m
          kubectl get pods -n application
          kubectl get svc -n application

      - name: Run smoke tests
        run: |
          API_URL=$(kubectl get svc api -n application -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          if [ -z "$API_URL" ]; then
            API_URL=$(kubectl get svc api -n application -o jsonpath='{.spec.clusterIP}')
          fi
          kubectl run curl-test --image=curlimages/curl:latest --rm -i --restart=Never -- \
            curl -f http://$API_URL:80/health || exit 1
  # ============= GITOPS SYNC (Optional) =============
  sync-argocd:
    name: Sync ArgoCD Applications
    needs: build-and-push
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install ArgoCD CLI
        run: |
          curl -sSL -o argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
          chmod +x argocd
          sudo mv argocd /usr/local/bin/

      - name: Login to ArgoCD
        run: |
          argocd login ${{ secrets.ARGOCD_SERVER }} \
            --username admin \
            --password ${{ secrets.ARGOCD_PASSWORD }} \
            --insecure

      - name: Sync applications
        run: |
          argocd app sync ip-geolocation-api --async
          argocd app sync monitoring-stack --async

      - name: Wait for sync
        run: |
          argocd app wait ip-geolocation-api --timeout 300
          argocd app wait monitoring-stack --timeout 300
